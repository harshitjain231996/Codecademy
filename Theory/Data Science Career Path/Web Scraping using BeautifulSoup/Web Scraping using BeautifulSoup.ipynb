{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "In Data Science, we can do a lot of exciting work with the right dataset. Once we have interesting data, we can use Pandas or Matplotlib to analyze or visualize trends. But how do we get that data in the first place?\n",
    "\n",
    "If it’s provided to us in a well-organized csv or json file, we’re lucky! Most of the time, we need to go out and search for it ourselves.\n",
    "\n",
    "Often times you’ll find the perfect website that has all the data you need, but there’s no way to download it. This is where BeautifulSoup comes in handy to scrape the HTML. If we find the data we want to analyze online, we can use BeautifulSoup to grab it and turn it into a structure we can understand. This Python library, allows us to easily and quickly take information from a website and put it into a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules of Scraping\n",
    "\n",
    "When we scrape websites, we have to make sure we are following some guidelines so that we are treating the websites and their owners with respect.\n",
    "\n",
    "Always check a website’s Terms and Conditions before scraping. Read the statement on the legal use of data. Usually, the data you scrape should not be used for commercial purposes.\n",
    "\n",
    "Do not spam the website with a ton of requests. A large number of requests can break a website that is unprepared for that level of traffic. As a general rule of good practice, make one request to one webpage per second.\n",
    "\n",
    "If the layout of the website changes, you will have to change your scraping code to follow the new structure of the site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requests\n",
    "\n",
    "In order to get the HTML of the website, we need to make a request to get the content of the webpage. \n",
    "\n",
    "Python has a requests library that makes getting content really easy. All we have to do is import the library, and then feed in the URL we want to GET:\n",
    "\n",
    "import requests\n",
    "\n",
    "webpage = requests.get('https://www.codecademy.com/articles/http-requests')\n",
    "\n",
    "print(webpage.text)\n",
    "\n",
    "This code will print out the HTML of the page.\n",
    "\n",
    "![r1](https://i.imgur.com/CDkzv5A.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The BeautifulSoup Object\n",
    "\n",
    "When we printed out all of that HTML from our request, it seemed pretty long and messy. How could we pull out the relevant information from that long string?\n",
    "\n",
    "BeautifulSoup is a Python library that makes it easy for us to traverse an HTML page and pull out the parts we’re interested in. We can import it by using the line:\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "Then, all we have to do is convert the HTML document to a BeautifulSoup object!\n",
    "\n",
    "If this is our HTML file, rainbow.html:\n",
    "\n",
    "![bs1](https://i.imgur.com/nHExCMm.jpg)\n",
    "\n",
    "\"html.parser\" is one option for parsers we could use. There are other options, like \"lxml\" and \"html5lib\" that have different advantages and disadvantages, but for our purposes we will be using \"html.parser\" throughout.\n",
    "\n",
    "With the requests skills we just learned, we can use a website hosted online as that HTML:\n",
    "\n",
    "webpage = requests.get(\"http://rainbow.com/rainbow.html\", \"html.parser\")\n",
    "\n",
    "soup = BeautifulSoup(webpage.content)\n",
    "\n",
    "When we use BeautifulSoup in combination with pandas, we can turn websites into DataFrames that are easy to manipulate and gain insights from.\n",
    "\n",
    "![bs2](https://i.imgur.com/NZ6mEDk.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Types\n",
    "\n",
    "BeautifulSoup breaks the HTML page into several types of objects.\n",
    "\n",
    "### Tags\n",
    "\n",
    "![obt2](https://i.imgur.com/QqhRZ5f.jpg)\n",
    "\n",
    "div\n",
    "\n",
    "{'id': 'example'}\n",
    "\n",
    "### NavigableStrings\n",
    "\n",
    "NavigableStrings are the pieces of text that are in the HTML tags on the page. You can get the string inside of the tag by calling .string:\n",
    "\n",
    "print(soup.div.string)\n",
    "\n",
    "An example div\n",
    "\n",
    "![obt](https://i.imgur.com/HwewhtK.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigating by Tags\n",
    "\n",
    "![tg1](https://i.imgur.com/72kCwOi.jpg)\n",
    "\n",
    "![tg2](https://i.imgur.com/XaBxTuw.jpg)\n",
    "\n",
    "![tg3](https://i.imgur.com/6mJmbGv.jpg)\n",
    "\n",
    "![tg4](https://i.imgur.com/SzD7N41.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Website Structure\n",
    "\n",
    "When we’re telling our Python script what HTML tags to grab, we need to know the structure of the website and what we’re looking for.\n",
    "\n",
    "When you’re preparing to scrape a website, first inspect the HTML to see where the info you are looking for is located on the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find All\n",
    "\n",
    "If we want to find all of the occurrences of a tag, instead of just the first one, we can use .find_all().\n",
    "\n",
    "This function can take in just the name of a tag and returns a list of all occurrences of that tag.\n",
    "\n",
    "![fa1](https://i.imgur.com/cvpIE3r.jpg)\n",
    "\n",
    "![fa2](https://i.imgur.com/k5diJik.jpg)\n",
    "\n",
    "![fa3](https://i.imgur.com/gclWjSr.jpg)\n",
    "\n",
    "### Using A Function\n",
    "\n",
    "If our selection starts to get really complicated, we can separate out all of the logic that we’re using to choose a tag into its own function. Then, we can pass that function into .find_all()!\n",
    "\n",
    "![fa4](https://i.imgur.com/8gMJUzw.jpg)\n",
    "\n",
    "![fa5](https://i.imgur.com/67DbwjX.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select for CSS Selectors\n",
    "\n",
    "![cs1](https://i.imgur.com/Y0uvenZ.jpg)\n",
    "\n",
    "![cs2](https://i.imgur.com/Fu2Ds5Z.jpg)\n",
    "\n",
    "![cs3](https://i.imgur.com/K9RAs23.jpg)\n",
    "\n",
    "![cs4](https://i.imgur.com/bn0DhIs.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Text\n",
    "\n",
    "![rt3](https://i.imgur.com/CtPebCe.jpg)\n",
    "\n",
    "![rt4](https://i.imgur.com/A6MRMZi.jpg)\n",
    "\n",
    "![rt1](https://i.imgur.com/mLE2NXe.jpg)\n",
    "\n",
    "![rt2](https://i.imgur.com/DEIO56L.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "![re1](https://i.imgur.com/rCKZxuq.jpg)\n",
    "\n",
    "![re2](https://i.imgur.com/oc4QafY.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "![q1](https://i.imgur.com/ICBUbI2.jpg)\n",
    "\n",
    "![q2](https://i.imgur.com/qjgnHmy.jpg)\n",
    "\n",
    "![q3](https://i.imgur.com/PVUplgV.jpg)\n",
    "\n",
    "![q4](https://i.imgur.com/f6l3DUY.jpg)\n",
    "\n",
    "![q5](https://i.imgur.com/pH1aZml.jpg)\n",
    "\n",
    "![q6](https://i.imgur.com/FJI92fP.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
