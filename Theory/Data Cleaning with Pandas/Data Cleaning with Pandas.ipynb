{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "A huge part of data science involves acquiring raw data and getting it into a form ready for analysis. Some have estimated that data scientists spend 80% of their time cleaning and manipulating data, and only 20% of their time actually analyzing it or building models from it.\n",
    "\n",
    "When we receive raw data, we have to do a number of things before we’re ready to analyze it, possibly including:\n",
    "\n",
    "1. diagnosing the “tidiness” of the data — how much data cleaning we will have to do\n",
    "2. reshaping the data — getting right rows and columns for effective analysis\n",
    "3. combining multiple files\n",
    "4. changing the types of values — how we fix a column where numerical values are stored as strings, for example\n",
    "5. dropping or filling missing values - how we deal with data that is incomplete or missing\n",
    "6. manipulating strings to represent the data better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnose the Data\n",
    "\n",
    "We often describe data that is easy to analyze and visualize as “tidy data”. What does it mean to have tidy data?\n",
    "\n",
    "For data to be tidy, it must have:\n",
    "\n",
    "1. Each variable as a separate column\n",
    "2. Each row as a separate observation\n",
    "\n",
    "The first step of diagnosing whether or not a dataset is tidy is using pandas functions to explore and probe the dataset.\n",
    "\n",
    "You’ve seen most of the functions we often use to diagnose a dataset for cleaning. Some of the most useful ones are:\n",
    "\n",
    "1. .head() — display the first 5 rows of the table\n",
    "2. .info() — display a summary of the table\n",
    "3. .describe() — display the summary statistics of the table\n",
    "4. .columns — display the column names of the table\n",
    "5. .value_counts() — display the distinct values for a column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Multiple Files\n",
    "\n",
    "Often, you have the same data separated out into multiple files.\n",
    "\n",
    "Let’s say that we have a ton of files following the filename structure: 'file1.csv', 'file2.csv', 'file3.csv', and so on. The power of pandas is mainly in being able to manipulate large amounts of structured data, so we want to be able to get all of the relevant information into one table so that we can analyze the aggregate data.\n",
    "\n",
    "We can combine the use of glob, a Python library for working with files, with pandas to organize this data better. glob can open multiple files by using regex matching to get the filenames:\n",
    "\n",
    "import glob\n",
    "\n",
    "files = glob.glob(\"file*.csv\")\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for filename in files:\n",
    "\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    df_list.append(data)\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "\n",
    "print(files)\n",
    "\n",
    "This code goes through any file that starts with 'file' and has an extension of .csv. It opens each file, reads the data into a DataFrame, and then concatenates all of those DataFrames together.\n",
    "\n",
    "![mf1](https://i.imgur.com/TIU8L55.jpg)\n",
    "\n",
    "![mf2](https://i.imgur.com/6ONqa8B.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping your Data\n",
    "\n",
    "Since we want\n",
    "\n",
    "1. Each variable as a separate column\n",
    "2. Each row as a separate observation\n",
    "\n",
    "We can use pd.melt() to do this transformation. .melt() takes in a DataFrame, and the columns to unpack.\n",
    "\n",
    "The parameters you provide are:\n",
    "\n",
    "1. frame: the DataFrame you want to melt\n",
    "2. id_vars: the column(s) of the old DataFrame to preserve\n",
    "3. value_vars: the column(s) of the old DataFrame that you want to turn into variables\n",
    "4. value_name: what to call the column of the new DataFrame that stores the values\n",
    "5. var_name: what to call the column of the new DataFrame that stores the variables\n",
    "\n",
    "The default names may work in certain situations, but it’s best to always have data that is self-explanatory. Thus, we often use .columns() to rename the columns after melting.\n",
    "\n",
    "![rsd](https://i.imgur.com/otV0Hcx.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Duplicates\n",
    "\n",
    "Often we see duplicated rows of data in the DataFrames we are working with. This could happen due to errors in data collection or in saving and loading the data.\n",
    "\n",
    "To check for duplicates, we can use the pandas function .duplicated(), which will return a Series telling us which rows are duplicate rows.\n",
    "\n",
    "We can use the pandas .drop_duplicates() function to remove all rows that are duplicates of another row.\n",
    "\n",
    "If we wanted to remove every row with a duplicate value in the item column, we could specify a subset:\n",
    "\n",
    "fruits = fruits.drop_duplicates(subset=['item'])\n",
    "\n",
    "Make sure that the columns you drop duplicates from are specifically the ones where duplicates don’t belong. You wouldn’t want to drop duplicates with the price column as a subset, for example, because it’s okay if multiple items cost the same amount!\n",
    "\n",
    "![dup](https://i.imgur.com/mlQDWc7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting by Index\n",
    "\n",
    "In trying to get clean data, we want to make sure each column represents one type of measurement. Often, multiple measurements are recorded in the same column, and we want to separate these out so that we can do individual analysis on each variable.\n",
    "\n",
    "Let’s say we have a column “birthday” with data formatted in MMDDYYYY format. In other words, “11011993” represents a birthday of November 1, 1993. We want to split this data into day, month, and year so that we can use these columns as separate features.\n",
    "\n",
    "In this case, we know the exact structure of these strings. The first two characters will always correspond to the month, the second two to the day, and the rest of the string will always correspond to year. We can easily break the data into three separate columns by splitting the strings using .str:\n",
    "\n",
    "df['month'] = df.birthday.str[0:2] # Create the 'month' column\n",
    "\n",
    "df['day'] = df.birthday.str[2:4] # Create the 'day' column\n",
    "\n",
    "df['year'] = df.birthday.str[4:] # Create the 'year' column\n",
    "\n",
    "The first command takes the first two characters of each value in the birthday column and puts it into a month column. The second command takes the second two characters of each value in the birthday column and puts it into a day column. The third command takes the rest of each value in the birthday column and puts it into a year column.\n",
    "\n",
    "![si1](https://i.imgur.com/jwFnP9X.jpg)\n",
    "\n",
    "![si2](https://i.imgur.com/Agx4f4y.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting by Character\n",
    "\n",
    "Let’s say we have a column called “type” with data entries in the format \"admin_US\" or \"user_Kenya\". Just like we saw before, this column actually contains two types of data. One seems to be the user type (with values like “admin” or “user”) and one seems to be the country this user is in (with values like “US” or “Kenya”).\n",
    "\n",
    "We can no longer just split along the first 4 characters because admin and user are of different lengths. Instead, we know that we want to split along the \"_\". Using that, we can split this column into two separate, cleaner columns:\n",
    "\n",
    "df['str_split'] = df.type.str.split('_') # Create the 'str_split' column\n",
    "\n",
    "df['usertype'] = df.str_split.str.get(0) # Create the 'usertype' column\n",
    "\n",
    "df['country'] = df.str_split.str.get(1) # Create the 'country' column\n",
    "\n",
    "![sc1](https://i.imgur.com/QnFon8G.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Types\n",
    "\n",
    "Each column of a DataFrame can hold items of the same data type or dtype. The dtypes that pandas uses are: float, int, bool, datetime, timedelta, category and object. Often, we want to convert between types so that we can do better analysis. If a numerical category like \"num_users\" is stored as a Series of objects instead of ints, for example, it makes it more difficult to do something like make a line graph of users over time.\n",
    "\n",
    "To see the types of each column of a DataFrame, we can use:\n",
    "\n",
    "print(df.dtypes)\n",
    "\n",
    "![t1](https://i.imgur.com/p2g3LfY.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Parsing\n",
    "\n",
    "Sometimes we need to modify strings in our DataFrames to help us transform them into more meaningful metrics. \n",
    "\n",
    "The 'price' column is actually composed of strings representing dollar amounts. This column could be much better represented in floats, so that we could take the mean, calculate other aggregate statistics, or compare different fruits to one another in terms of price.\n",
    "\n",
    "First, we can use what we know of regex to get rid of all of the dollar signs:\n",
    "\n",
    "fruit.price = fruit['price'].replace('[\\$,]', '', regex=True)\n",
    "\n",
    "Then, we can use the pandas function .to_numeric() to convert strings containing numerical values to integers or floats:\n",
    "\n",
    "fruit.price = pd.to_numeric(fruit.price)\n",
    "\n",
    "![sp1](https://i.imgur.com/vonR1M8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More String Parsing\n",
    "\n",
    "Sometimes we want to do analysis on numbers that are hidden within string values. We can use regex to extract this numerical data from the strings they are trapped in.\n",
    "\n",
    "To extract the numbers from the string we can use pandas’ .str.split() function:\n",
    "\n",
    "split_df = df['exerciseDescription'].str.split('(\\d+)', expand=True)\n",
    "\n",
    "Then, we can assign columns from this DataFrame to the original df:\n",
    "\n",
    "df.reps = pd.to_numeric(split_df[1])\n",
    "\n",
    "df.exercise = split_df[2].replace('[\\- ]', '', regex=True)\n",
    "\n",
    "![sp2](https://i.imgur.com/8Hz9CF5.jpg)\n",
    "\n",
    "![sp3](https://i.imgur.com/T5yc3na.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "\n",
    "We often have data with missing elements, as a result of a problem with the data collection process or errors in the way the data was stored. The missing elements normally show up as NaN (or Not a Number) values. \n",
    "\n",
    "### Method 1: drop all of the rows with a missing value\n",
    "\n",
    "We can use .dropna() to do this:\n",
    "\n",
    "bill_df = bill_df.dropna()\n",
    "\n",
    "This command will result in the DataFrame without the incomplete rows.\n",
    "\n",
    "### Method 2: fill the missing values with the mean of the column, or with some other aggregate value.\n",
    "\n",
    "We can use .fillna() to do this:\n",
    "\n",
    "bill_df = bill_df.fillna(value={\"bill\":bill_df.bill.mean(), \"num_guests\":bill_df.num_guests.mean()})\n",
    "\n",
    "This command will result in the DataFrame with the respective mean of the column in the place of the original NaNs.\n",
    "\n",
    "![mv1](https://i.imgur.com/Qg1qF4N.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "We have looked at a number of different methods we may use to get data into the format we want for analysis.\n",
    "\n",
    "Specifically, we have covered:\n",
    "\n",
    "1. diagnosing the “tidiness” of the data\n",
    "2. reshaping the data\n",
    "3. combining multiple files\n",
    "4. changing the types of values\n",
    "5. dropping or filling missing values - how we deal with data that is incomplete or missing\n",
    "6. manipulating strings to represent the data better\n",
    "\n",
    "You can use these methods to transform your datasets to be clean and easy to work with!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "![qd1](https://i.imgur.com/T4ekNxk.jpg)\n",
    "\n",
    "![qd2](https://i.imgur.com/AuHAtH0.jpg)\n",
    "\n",
    "![qd3](https://i.imgur.com/UbDxMws.jpg)\n",
    "\n",
    "![qd4](https://i.imgur.com/5BK0t9G.jpg)\n",
    "\n",
    "![qd5](https://i.imgur.com/nn2mAVt.jpg)\n",
    "\n",
    "![qd6](https://i.imgur.com/n3gGb9p.jpg)\n",
    "\n",
    "![qd7](https://i.imgur.com/OvSsG0Z.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
